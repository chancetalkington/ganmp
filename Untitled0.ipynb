{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1fkuWaR441tQHsClaF7SmLbx7ZdYmHYdT","authorship_tag":"ABX9TyN5MvrTpCP2c8yu+u+s+xCR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZvHDBH1kN7WR"},"source":["import os\n","import torch\n","import torchvision\n","import pandas as pd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm.notebook import tqdm\n","import torchvision.models as models\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split\n","from torchvision.utils import make_grid\n","import torchvision.transforms as transforms\n","from torchvision.datasets.folder import default_loader\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QiPQ0RyTN-1u","executionInfo":{"status":"ok","timestamp":1630181665902,"user_tz":360,"elapsed":4,"user":{"displayName":"chance talkington","photoUrl":"","userId":"10145936639393291826"}},"outputId":"576ca026-77af-41e4-e28c-8771573c76dd"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hxQGkxgfN-5F","executionInfo":{"status":"ok","timestamp":1630181666305,"user_tz":360,"elapsed":405,"user":{"displayName":"chance talkington","photoUrl":"","userId":"10145936639393291826"}},"outputId":"2615e596-f07e-43d4-e047-575b6a01ae9c"},"source":["with open('/content/drive/My Drive/foo.txt', 'w') as f:\n","  f.write('Hello Google Drive!')\n","!cat /content/drive/My\\ Drive/foo.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hello Google Drive!"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3LyiqDTN_Ae","executionInfo":{"status":"ok","timestamp":1630181668206,"user_tz":360,"elapsed":1902,"user":{"displayName":"chance talkington","photoUrl":"","userId":"10145936639393291826"}},"outputId":"50a60926-237e-4ff4-c364-d82292509f10"},"source":["## image_resizer.py\n","# Importing required libraries\n","import os\n","import numpy as np\n","from PIL import Image\n","\n","# Defining an image size and image channel\n","# We are going to resize all our images to 128X128 size and since our images are colored images\n","# We are setting our image channels to 3 (RGB)\n","\n","IMAGE_SIZE = 128\n","IMAGE_CHANNELS = 3\n","IMAGE_DIR = '/content/drive/MyDrive/warhol'\n","\n","# Defining image dir path. Change this if you have different directory\n","images_path = IMAGE_DIR \n","\n","training_data = []\n","\n","# Iterating over the images inside the directory and resizing them using\n","# Pillow's resize method.\n","print('resizing...')\n","\n","for filename in os.listdir(images_path):\n","    path = os.path.join(images_path, filename)\n","    image = Image.open(path).resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n","\n","    training_data.append(np.asarray(image))\n","\n","training_data = np.reshape(\n","    training_data, (-1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n","training_data = training_data / 127.5 - 1\n","\n","print('saving file...')\n","np.save('vangough.npy', training_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["resizing...\n","saving file...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ikJU33ElN_Du"},"source":["from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model, load_model\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","from PIL import Image\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGZUoJqIN_GW"},"source":["# Preview image Frame\n","PREVIEW_ROWS = 4\n","PREVIEW_COLS = 7\n","PREVIEW_MARGIN = 4\n","SAVE_FREQ = 100\n","# Size vector to generate images from\n","NOISE_SIZE = 100\n","# Configuration\n","EPOCHS = 10000 # number of iterations\n","BATCH_SIZE = 32\n","GENERATE_RES = 3\n","IMAGE_SIZE = 128 # rows/cols\n","IMAGE_CHANNELS = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKWolL7R4jeU"},"source":["training_data = np.load('vangough.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjLwBmtdSCvy"},"source":["def build_discriminator(image_shape):\n","    model = Sequential()\n","    model.add(Conv2D(32, kernel_size=3, strides=2,\n","    input_shape=image_shape, padding=\"same\"))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Flatten())\n","    model.add(Dense(1, activation='sigmoid'))\n","    input_image = Input(shape=image_shape)\n","    validity = model(input_image)\n","    return Model(input_image, validity)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ocu3nL7vSCx0"},"source":["def build_generator(noise_size, channels):\n","    model = Sequential()\n","    model.add(Dense(4 * 4 * 256, activation=\"relu\",       input_dim=noise_size))\n","    model.add(Reshape((4, 4, 256)))\n","    model.add(UpSampling2D())\n","    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Activation(\"relu\"))\n","    model.add(UpSampling2D())\n","    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Activation(\"relu\"))\n","    for i in range(GENERATE_RES):\n","         model.add(UpSampling2D())\n","         model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n","         model.add(BatchNormalization(momentum=0.8))\n","         model.add(Activation(\"relu\"))\n","    model.summary()\n","    model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n","    model.add(Activation(\"tanh\"))\n","    input = Input(shape=(noise_size,))\n","    generated_image = model(input)\n","    \n","    return Model(input, generated_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SmC9OxJRN_Ie"},"source":["def save_images(cnt, noise):\n","  image_array = np.full((\n","      PREVIEW_MARGIN + (PREVIEW_ROWS * (IMAGE_SIZE + PREVIEW_MARGIN)),\n","      PREVIEW_MARGIN + (PREVIEW_COLS * (IMAGE_SIZE + PREVIEW_MARGIN)), 3),\n","      255, dtype=np.uint8)\n","  \n","  generated_images = generator.predict(noise)\n","\n","  generated_images = 0.5 * generated_images + 0.5\n","\n","  image_count = 0\n","  for row in range(PREVIEW_ROWS) :\n","      for col in range(PREVIEW_COLS):\n","        r = row * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN\n","        c = col * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN\n","        image_array[r:r + IMAGE_SIZE, c:c +\n","                    IMAGE_SIZE] = generated_images[image_count] * 255\n","        image_count += 1\n","\n","  output_path = 'output'\n","  if not os.path.exists(output_path):\n","      os.makedirs(output_path)\n","\n","  filename = os.path.join(output_path, f\"trained-(cnt).png\")\n","  im = Image.fromarray(image_array)\n","  im.save(filename)\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NS2utYRcN_Ku","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630182894502,"user_tz":360,"elapsed":366593,"user":{"displayName":"chance talkington","photoUrl":"","userId":"10145936639393291826"}},"outputId":"5ebdf6ad-1c2b-4002-e597-32dd60124814"},"source":["image_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS)\n","\n","optimizer = Adam(1.5e-4, 0.5)\n","\n","discriminator = build_discriminator(image_shape)\n","discriminator.compile(loss=\"binary_crossentropy\",\n","optimizer=optimizer, metrics=[\"accuracy\"])\n","generator = build_generator(NOISE_SIZE, IMAGE_CHANNELS)\n","\n","random_input = Input(shape=(NOISE_SIZE,))\n","\n","generated_image = generator(random_input)\n","\n","discriminator.trainable = False\n","\n","validity = discriminator(generated_image)\n","\n","combined = Model(random_input, validity)\n","combined.compile(loss=\"binary_crossentropy\",\n","optimizer=optimizer, metrics=[\"accuracy\"])\n","\n","y_real = np.ones((BATCH_SIZE, 1))\n","y_fake = np.zeros((BATCH_SIZE, 1))\n","\n","fixed_noise = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, NOISE_SIZE))\n","\n","cnt = 1\n","for epoch in range(EPOCHS):\n"," idx = np.random.randint(0, training_data.shape[0], BATCH_SIZE)\n"," x_real = training_data[idx]\n"," \n"," noise= np.random.normal(0, 1, (BATCH_SIZE, NOISE_SIZE))\n"," x_fake = generator.predict(noise)\n"," \n"," discriminator_metric_real = discriminator.train_on_batch(x_real, y_real)\n","discriminator_metric_generated = discriminator.train_on_batch(\n"," x_fake, y_fake)\n"," \n","discriminator_metric = 0.5 * np.add(discriminator_metric_real, discriminator_metric_generated)\n","generator_metric = combined.train_on_batch(noise, y_real)\n","if epoch % SAVE_FREQ == 0:\n","   save_images(cnt, fixed_noise)\n","   cnt += 1\n"," \n","   print(f\"{epoch} epoch, Discriminator accuracy: {100*  discriminator_metric[1]}, Generator accuracy: {100 * generator_metric[1]}\")"],"execution_count":11,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 4096)              413696    \n","_________________________________________________________________\n","reshape (Reshape)            (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 8, 8, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n","_________________________________________________________________\n","activation (Activation)      (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","up_sampling2d_1 (UpSampling2 (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 16, 16, 256)       590080    \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 16, 16, 256)       1024      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","up_sampling2d_2 (UpSampling2 (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 32, 32, 256)       590080    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 32, 32, 256)       1024      \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","up_sampling2d_3 (UpSampling2 (None, 64, 64, 256)       0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 64, 64, 256)       590080    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 64, 64, 256)       1024      \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 64, 64, 256)       0         \n","_________________________________________________________________\n","up_sampling2d_4 (UpSampling2 (None, 128, 128, 256)     0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 128, 128, 256)     590080    \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 128, 128, 256)     1024      \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 128, 128, 256)     0         \n","=================================================================\n","Total params: 3,369,216\n","Trainable params: 3,366,656\n","Non-trainable params: 2,560\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"FZuHiX5wyqhA"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"XFbv1oh64w8_"},"source":["# New Section"]}]}